{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Remaining Useful Life (RUL) Model Pipeline\n",
        "This notebook builds a complete pipeline for training an LSTM model on the CMAPSS FD004 dataset.\n"
      ],
      "metadata": {
        "id": "EnHTcgP2giPM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXkRZHzDge_9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('train_FD004.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute RUL\n",
        "max_cycle = df.groupby('unit')['cycle'].max().reset_index().rename(columns={'cycle': 'max_cycle'})\n",
        "max_cycle\n",
        "#df = df.merge(max_cycle, on='unit', how='left')\n",
        "\n",
        "#df['RUL'] = df['max_cycle'] - df['cycle']\n",
        "#df[['unit','cycle','RUL']].head()\n"
      ],
      "metadata": {
        "id": "TEH5FJ_Rgqlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection\n",
        "exclude = {'unit','cycle','RUL','max_cycle'}\n",
        "feature_cols = [c for c in df.columns if c not in exclude]\n",
        "features = df[feature_cols].values\n",
        "labels = df['RUL'].values\n",
        "\n",
        "# Scaling\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "scaled_df = pd.DataFrame(scaled_features, columns=feature_cols)\n",
        "scaled_df['RUL'] = labels\n",
        "scaled_df['unit'] = df['unit']\n",
        "scaled_df['cycle'] = df['cycle']\n",
        "scaled_df.head()"
      ],
      "metadata": {
        "id": "iAytkzypgtJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequence building\n",
        "TIME_STEPS = 50\n",
        "X_list, y_list = [], []\n",
        "for uid, group in scaled_df.groupby('unit'):\n",
        "    g = group[feature_cols + ['RUL']].values\n",
        "    if len(g) >= TIME_STEPS:\n",
        "        for i in range(len(g) - TIME_STEPS + 1):\n",
        "            X_list.append(g[i:i+TIME_STEPS, :-1])  # sensors\n",
        "            y_list.append(g[i+TIME_STEPS-1, -1])   # RUL at end\n",
        "\n",
        "X = np.array(X_list)\n",
        "y = np.array(y_list)\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "ImMFr8T5gvj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "Lu3cXwgGgyBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build LSTM model\n",
        "tf.keras.backend.clear_session()\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(loss='mse', optimizer=Adam(1e-3), metrics=['mae'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "asGYABJgg0Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "ma3k0VJJg2XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(X_test).reshape(-1)\n",
        "y_true = y_test.reshape(-1)\n",
        "\n",
        "print(\"Pred shape:\", pred.shape)\n",
        "print(\"True shape:\", y_true.shape)\n"
      ],
      "metadata": {
        "id": "GgZTnDLRhAdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
        "plt.title(\"Training & Validation Loss (FD004)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bTffryPOhChR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 200\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(y_true[:N], label=\"True RUL\")\n",
        "plt.plot(pred[:N], label=\"Predicted RUL\")\n",
        "plt.title(\"True vs Predicted RUL (First 200 Samples) - FD004\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"RUL\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WZmMOT2BhE0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5.5,5.5))\n",
        "plt.scatter(y_true, pred, alpha=0.35)\n",
        "\n",
        "lo = min(y_true.min(), pred.min())\n",
        "hi = max(y_true.max(), pred.max())\n",
        "\n",
        "plt.plot([lo, hi], [lo, hi], \"--\")\n",
        "plt.title(\"Predicted vs True RUL - FD004\")\n",
        "plt.xlabel(\"True RUL\")\n",
        "plt.ylabel(\"Predicted RUL\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MXIThixkhHPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 7. evaluate --------\n",
        "# Typical MAE for FD001 using a basic LSTM:\n",
        "# \t•\tMAE < 20 → Excellent\n",
        "# \t•\tMAE 20–30 → Good\n",
        "# \t•\tMAE 30–50 → Acceptable\n",
        "# \t•\tMAE > 50 → Poor model performance\n",
        "mae = np.mean(np.abs(y_true - pred))\n",
        "rmse = np.sqrt(np.mean((y_true - pred)**2))\n",
        "mse = np.mean((y_true - pred)**2)\n",
        "\n",
        "print(\"\\n=== FD004 Test Metrics ===\")\n",
        "print(f\"MAE : {mae:.3f}\")\n",
        "print(f\"MSE : {mse:.3f}\")\n",
        "print(f\"RMSE: {rmse:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "E95PKku6hJ6p",
        "outputId": "3c07409e-7058-40ec-b77a-a6cc3adf5c60"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3217012512.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#       •       MAE 30–50 → Acceptable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#       •       MAE > 50 → Poor model performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #1 Correlation with RUL (Fast Diagnostic)\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sensor_cols = [c for c in df.columns if \"sensor\" in c]\n",
        "\n",
        "corrs = df[sensor_cols + [\"RUL\"]].corr()[\"RUL\"].abs().sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.barplot(x=corrs.values, y=corrs.index)\n",
        "plt.title(\"Sensor–RUL Correlation Importance\")\n",
        "plt.xlabel(\"Absolute Correlation with RUL\")\n",
        "plt.ylabel(\"Sensor\")\n",
        "plt.show()\n",
        "\n",
        "print(corrs)"
      ],
      "metadata": {
        "id": "1uNKE-oVhTUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1 Correlation with RUL (Fast Diagnostic)\n",
        "corr = df.corr()['RUL'].abs().sort_values(ascending=False)\n",
        "corr.head(15)"
      ],
      "metadata": {
        "id": "tBTaVCXBhV9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2: Time Series Stability / Degradation Analysis\n",
        "\n",
        "# Some sensors don’t change at all (flat line) → useless\n",
        "# Some sensors vary but not related to failure → noise\n",
        "# Some sensors show consistent degradation → extremely important\n",
        "\n",
        "# Identify sensors with clear downward or upward trends\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "slopes = {}\n",
        "\n",
        "for s in sensor_cols:\n",
        "    slopes[s] = np.polyfit(df[\"cycle\"], df[s], 1)[0]\n",
        "\n",
        "pd.Series(slopes).sort_values()\n",
        "\n",
        "\t# •\tLarge negative slope = strong degradation\n",
        "\t# •\tLarge positive slope = increasing stress/temp etc → can also be important\n",
        "\t# •\tValues near zero = poor signal"
      ],
      "metadata": {
        "id": "BFnrPSEUhYoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Permutation Importance\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def permutation_importance_lstm(model, X_val, y_val, feature_names):\n",
        "    baseline_pred = model.predict(X_val)\n",
        "    baseline_mae = mean_absolute_error(y_val, baseline_pred)\n",
        "\n",
        "    importance = {}\n",
        "\n",
        "    for i, name in enumerate(feature_names):\n",
        "        X_shuffled = X_val.copy()\n",
        "\n",
        "        # shuffle only one feature across all time steps\n",
        "        X_shuffled[:,:,i] = np.random.permutation(X_shuffled[:,:,i].flatten()).reshape(X_val[:,:,i].shape)\n",
        "\n",
        "        pred = model.predict(X_shuffled)\n",
        "        mae = mean_absolute_error(y_val, pred)\n",
        "        importance[name] = mae - baseline_mae\n",
        "\n",
        "    return sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "important_sensors = permutation_importance_lstm(model, X_test, y_test, feature_cols)\n",
        "important_sensors[:10]"
      ],
      "metadata": {
        "id": "UOf0f_q-ha0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.PCA / Health Indicator Contribution\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=3)\n",
        "pca.fit(df[feature_cols])\n",
        "\n",
        "importance = pd.Series(np.abs(pca.components_[0]), index=feature_cols)\n",
        "importance.sort_values(ascending=False).head(10)"
      ],
      "metadata": {
        "id": "T3Ly5PKxhdSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key_sensors = [\"sensor_9\", \"sensor_18\", \"sensor_8\", \"sensor_12\", \"sensor_14\", \"sensor_7\", \"sensor_3\", \"sensor_4\", \"sensor_13\", \"sensor_2\"]"
      ],
      "metadata": {
        "id": "Eb09-szDhfxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "weights = {\n",
        "    'sensor_9':\t0.712440,\n",
        "'sensor_18':\t0.285501,\n",
        "'sensor_8':\t0.285324,\n",
        "'sensor_7':\t0.281067,\n",
        "'sensor_12':\t0.265165,\n",
        "'sensor_4':\t0.248125,\n",
        "'sensor_3':\t0.223987,\n",
        "'sensor_13':\t0.184424,\n",
        "'sensor_14':\t0.146053,\n",
        "'sensor_2':\t0.076381\n",
        "}\n",
        "\n",
        "df[\"CDI_weighted\"] = sum(df[s] * w for s, w in weights.items())"
      ],
      "metadata": {
        "id": "4EYRUKaahjOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_norm = (df[key_sensors] - df[key_sensors].min()) / (df[key_sensors].max() - df[key_sensors].min())\n",
        "df[\"HI_norm\"] = df_norm.mean(axis=1)\n",
        "df_norm"
      ],
      "metadata": {
        "id": "ubWnGernhlLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA-Based Health Index\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "X = df[key_sensors]\n",
        "\n",
        "# Standardize (PCA works best on standardized data)\n",
        "scaler_hi = StandardScaler()\n",
        "X_scaled = scaler_hi.fit_transform(X)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=1)\n",
        "df[\"HI_pca\"] = pca.fit_transform(X_scaled)\n",
        "\n",
        "df[\"HI_pca\"]"
      ],
      "metadata": {
        "id": "UIZALrD9hney"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "unit_id = 8\n",
        "g = df[df[\"unit\"] == unit_id]\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "#plt.plot(g[\"cycle\"], g[\"HI_pca\"], label=\"PCA Health Index\")\n",
        "#plt.plot(g[\"cycle\"], g[\"CDI_weighted\"], label=\"Weighted CDI\", alpha=0.7)\n",
        "plt.plot(g[\"cycle\"], g[\"HI_norm\"], label=\"Normalized HI\", alpha=0.7)\n",
        "\n",
        "plt.gca().invert_yaxis()        # health decreases over time\n",
        "plt.title(f\"Health Index for Engine Unit {unit_id}\")\n",
        "plt.xlabel(\"Cycle\")\n",
        "plt.ylabel(\"Health Indicator\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DlPMhu3RhpiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def fft_energy(x):\n",
        "    \"\"\"\n",
        "    Sum of squared magnitudes of the FFT (a simple measure of signal power).\n",
        "    x is a 1D numpy array (rolling window of one sensor).\n",
        "    \"\"\"\n",
        "    # real FFT (only non-negative frequencies)\n",
        "    f = np.fft.rfft(x)\n",
        "    mag = np.abs(f)\n",
        "    return float(np.sum(mag ** 2))\n",
        "\n",
        "def fft_peak(x):\n",
        "    \"\"\"\n",
        "    Maximum magnitude in the FFT spectrum (dominant amplitude).\n",
        "    \"\"\"\n",
        "    f = np.fft.rfft(x)\n",
        "    mag = np.abs(f)\n",
        "    return float(np.max(mag))"
      ],
      "metadata": {
        "id": "OBKw9VZJhrzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Important sensors (from your feature importance results)\n",
        "fft_sensors = [\"sensor_9\", \"sensor_18\", \"sensor_8\", \"sensor_12\", \"sensor_13\", \"sensor_7\", \"sensor_3\", \"sensor_4\", \"sensor_14\", \"sensor_2\"]\n",
        "\n",
        "# Rolling window (in cycles) for FFT computation\n",
        "FFT_WINDOW = 50  # you can try 30, 50, 100 etc."
      ],
      "metadata": {
        "id": "oGaIn4ivhuGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll compute FFT features per engine (unit) over a rolling window\n",
        "for col in fft_sensors:\n",
        "    energy_col = f\"{col}_fft_energy\"\n",
        "    peak_col   = f\"{col}_fft_peak\"\n",
        "\n",
        "    # Rolling by engine unit\n",
        "    df[energy_col] = (\n",
        "        df.groupby(\"unit\")[col]\n",
        "          .rolling(window=FFT_WINDOW, min_periods=FFT_WINDOW)\n",
        "          .apply(fft_energy, raw=True)\n",
        "          .reset_index(level=0, drop=True)\n",
        "    )\n",
        "\n",
        "    df[peak_col] = (\n",
        "        df.groupby(\"unit\")[col]\n",
        "          .rolling(window=FFT_WINDOW, min_periods=FFT_WINDOW)\n",
        "          .apply(fft_peak, raw=True)\n",
        "          .reset_index(level=0, drop=True)\n",
        "    )\n",
        "\n",
        "# Optional: check new columns\n",
        "#df[[ \"unit\", \"cycle\" ] + [s + \"_fft_energy\" for s in fft_sensors]].head(60)"
      ],
      "metadata": {
        "id": "l-lZ6pQ2hwaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unit_id = 8  # choose any engine ID that exists in your data\n",
        "plot_cols = [\"sensor_18\", \"sensor_18_fft_energy\", \"sensor_18_fft_peak\"]\n",
        "\n",
        "tmp = df[df[\"unit\"] == unit_id].reset_index(drop=True)\n",
        "\n",
        "ax = tmp.plot(x=\"cycle\", y=\"sensor_18\", label=\"sensor_18 (time)\", figsize=(8,6))\n",
        "ax2 = ax.twinx()\n",
        "tmp.plot(x=\"cycle\", y=\"sensor_18_fft_energy\", label=\"sensor_18_fft_energy\", ax=ax2, style='r--')\n",
        "tmp.plot(x=\"cycle\", y=\"sensor_18_fft_peak\", label=\"sensor_18_fft_peak\", ax=ax2, style='g:')\n",
        "\n",
        "ax.set_ylabel(\"Raw sensor_18\")\n",
        "ax2.set_ylabel(\"FFT features\")\n",
        "ax.set_title(f\"Engine {unit_id}: sensor_18 & FFT features\")\n",
        "ax.legend(loc=\"upper left\")\n",
        "ax2.legend(loc=\"upper right\")"
      ],
      "metadata": {
        "id": "cdo12NoJhyhj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}