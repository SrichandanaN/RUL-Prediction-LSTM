{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Remaining Useful Life (RUL) Model Pipeline\n",
        "This notebook builds a complete pipeline for training an LSTM model on the CMAPSS FD002 dataset.\n"
      ],
      "metadata": {
        "id": "1w2tD563e5nG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbeA1m3weztK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('train_FD002.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute RUL\n",
        "max_cycle = df.groupby('unit')['cycle'].max().reset_index().rename(columns={'cycle': 'max_cycle'})\n",
        "max_cycle\n",
        "#df = df.merge(max_cycle, on='unit', how='left')\n",
        "\n",
        "#df['RUL'] = df['max_cycle'] - df['cycle']\n",
        "#df[['unit','cycle','RUL']].head()\n"
      ],
      "metadata": {
        "id": "WaYE8jmae-GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection\n",
        "exclude = {'unit','cycle','RUL','max_cycle'}\n",
        "feature_cols = [c for c in df.columns if c not in exclude]\n",
        "features = df[feature_cols].values\n",
        "labels = df['RUL'].values\n",
        "\n",
        "# Scaling\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "scaled_df = pd.DataFrame(scaled_features, columns=feature_cols)\n",
        "scaled_df['RUL'] = labels\n",
        "scaled_df['unit'] = df['unit']\n",
        "scaled_df['cycle'] = df['cycle']\n",
        "scaled_df.head()"
      ],
      "metadata": {
        "id": "IoSswUmlfA8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequence building\n",
        "TIME_STEPS = 50\n",
        "X_list, y_list = [], []\n",
        "for uid, group in scaled_df.groupby('unit'):\n",
        "    g = group[feature_cols + ['RUL']].values\n",
        "    if len(g) >= TIME_STEPS:\n",
        "        for i in range(len(g) - TIME_STEPS + 1):\n",
        "            X_list.append(g[i:i+TIME_STEPS, :-1])  # sensors\n",
        "            y_list.append(g[i+TIME_STEPS-1, -1])   # RUL at end\n",
        "\n",
        "X = np.array(X_list)\n",
        "y = np.array(y_list)\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "ykU4QbsgfDOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "DMxawyi0fGDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build LSTM model\n",
        "tf.keras.backend.clear_session()\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(loss='mse', optimizer=Adam(1e-3), metrics=['mae'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7bdWoefEfJDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "2_3P5UNpfLDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(X_test).reshape(-1)\n",
        "y_true = y_test.reshape(-1)\n",
        "\n",
        "print(\"Pred shape:\", pred.shape)\n",
        "print(\"True shape:\", y_true.shape)\n"
      ],
      "metadata": {
        "id": "zSQ3T8ZzfUle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
        "plt.title(\"Training & Validation Loss (FD002)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_0CiHWFPfWjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 200\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(y_true[:N], label=\"True RUL\")\n",
        "plt.plot(pred[:N], label=\"Predicted RUL\")\n",
        "plt.title(\"True vs Predicted RUL (First 200 Samples) - FD002\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"RUL\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8Z1kA4yFfZFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5.5,5.5))\n",
        "plt.scatter(y_true, pred, alpha=0.35)\n",
        "\n",
        "lo = min(y_true.min(), pred.min())\n",
        "hi = max(y_true.max(), pred.max())\n",
        "\n",
        "plt.plot([lo, hi], [lo, hi], \"--\")\n",
        "plt.title(\"Predicted vs True RUL - FD002\")\n",
        "plt.xlabel(\"True RUL\")\n",
        "plt.ylabel(\"Predicted RUL\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZApvyKf8fbbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 7. evaluate --------\n",
        "# Typical MAE for FD001 using a basic LSTM:\n",
        "# \t•\tMAE < 20 → Excellent\n",
        "# \t•\tMAE 20–30 → Good\n",
        "# \t•\tMAE 30–50 → Acceptable\n",
        "# \t•\tMAE > 50 → Poor model performance\n",
        "mae = np.mean(np.abs(y_true - pred))\n",
        "rmse = np.sqrt(np.mean((y_true - pred)**2))\n",
        "mse = np.mean((y_true - pred)**2)\n",
        "\n",
        "print(\"\\n=== FD002 Test Metrics ===\")\n",
        "print(f\"MAE : {mae:.3f}\")\n",
        "print(f\"MSE : {mse:.3f}\")\n",
        "print(f\"RMSE: {rmse:.3f}\")\n"
      ],
      "metadata": {
        "id": "QQ__bSmdff1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #1 Correlation with RUL (Fast Diagnostic)\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sensor_cols = [c for c in df.columns if \"sensor\" in c]\n",
        "\n",
        "corrs = df[sensor_cols + [\"RUL\"]].corr()[\"RUL\"].abs().sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.barplot(x=corrs.values, y=corrs.index)\n",
        "plt.title(\"Sensor–RUL Correlation Importance\")\n",
        "plt.xlabel(\"Absolute Correlation with RUL\")\n",
        "plt.ylabel(\"Sensor\")\n",
        "plt.show()\n",
        "\n",
        "print(corrs)"
      ],
      "metadata": {
        "id": "ekr9n57bfiQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1 Correlation with RUL (Fast Diagnostic)\n",
        "corr = df.corr()['RUL'].abs().sort_values(ascending=False)\n",
        "corr.head(15)"
      ],
      "metadata": {
        "id": "7v8swqWNfi_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2: Time Series Stability / Degradation Analysis\n",
        "\n",
        "# Some sensors don’t change at all (flat line) → useless\n",
        "# Some sensors vary but not related to failure → noise\n",
        "# Some sensors show consistent degradation → extremely important\n",
        "\n",
        "# Identify sensors with clear downward or upward trends\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "slopes = {}\n",
        "\n",
        "for s in sensor_cols:\n",
        "    slopes[s] = np.polyfit(df[\"cycle\"], df[s], 1)[0]\n",
        "\n",
        "pd.Series(slopes).sort_values()\n",
        "\n",
        "\t# •\tLarge negative slope = strong degradation\n",
        "\t# •\tLarge positive slope = increasing stress/temp etc → can also be important\n",
        "\t# •\tValues near zero = poor signal"
      ],
      "metadata": {
        "id": "xxQR6ak0fl4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Permutation Importance\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def permutation_importance_lstm(model, X_val, y_val, feature_names):\n",
        "    baseline_pred = model.predict(X_val)\n",
        "    baseline_mae = mean_absolute_error(y_val, baseline_pred)\n",
        "\n",
        "    importance = {}\n",
        "\n",
        "    for i, name in enumerate(feature_names):\n",
        "        X_shuffled = X_val.copy()\n",
        "\n",
        "        # shuffle only one feature across all time steps\n",
        "        X_shuffled[:,:,i] = np.random.permutation(X_shuffled[:,:,i].flatten()).reshape(X_val[:,:,i].shape)\n",
        "\n",
        "        pred = model.predict(X_shuffled)\n",
        "        mae = mean_absolute_error(y_val, pred)\n",
        "        importance[name] = mae - baseline_mae\n",
        "\n",
        "    return sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "important_sensors = permutation_importance_lstm(model, X_test, y_test, feature_cols)\n",
        "important_sensors[:10]"
      ],
      "metadata": {
        "id": "WXBf_E0nfoFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.PCA / Health Indicator Contribution\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=3)\n",
        "pca.fit(df[feature_cols])\n",
        "\n",
        "importance = pd.Series(np.abs(pca.components_[0]), index=feature_cols)\n",
        "importance.sort_values(ascending=False).head(10)"
      ],
      "metadata": {
        "id": "BfVdc_0MfstW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key_sensors = [\"sensor_9\", \"sensor_18\", \"sensor_8\", \"sensor_12\", \"sensor_14\", \"sensor_7\", \"sensor_3\", \"sensor_4\", \"sensor_13\", \"sensor_2\"]"
      ],
      "metadata": {
        "id": "sS2mINxhfvEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "weights = {\n",
        "    'sensor_9':\t0.712347,\n",
        "'sensor_18':\t0.286250,\n",
        "'sensor_8':\t0.286094,\n",
        "'sensor_7':\t0.280184,\n",
        "'sensor_12':\t0.264340,\n",
        "'sensor_4':\t0.248423,\n",
        "'sensor_3':\t0.224224,\n",
        "'sensor_13':\t0.185068,\n",
        "'sensor_14':\t0.144903,\n",
        "'sensor_2':\t0.076542\n",
        "}\n",
        "\n",
        "df[\"CDI_weighted\"] = sum(df[s] * w for s, w in weights.items())"
      ],
      "metadata": {
        "id": "3pVEA-_Df06a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_norm = (df[key_sensors] - df[key_sensors].min()) / (df[key_sensors].max() - df[key_sensors].min())\n",
        "df[\"HI_norm\"] = df_norm.mean(axis=1)\n",
        "df_norm"
      ],
      "metadata": {
        "id": "LTFdfrUbf3ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA-Based Health Index\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "X = df[key_sensors]\n",
        "\n",
        "# Standardize (PCA works best on standardized data)\n",
        "scaler_hi = StandardScaler()\n",
        "X_scaled = scaler_hi.fit_transform(X)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=1)\n",
        "df[\"HI_pca\"] = pca.fit_transform(X_scaled)\n",
        "\n",
        "df[\"HI_pca\"]"
      ],
      "metadata": {
        "id": "fKMaZUBUf5o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "unit_id = 8\n",
        "g = df[df[\"unit\"] == unit_id]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "#plt.plot(g[\"cycle\"], g[\"HI_pca\"], label=\"PCA Health Index\")\n",
        "#plt.plot(g[\"cycle\"], g[\"CDI_weighted\"], label=\"Weighted CDI\", alpha=0.7)\n",
        "plt.plot(g[\"cycle\"], g[\"HI_norm\"], label=\"Normalized HI\", alpha=0.7)\n",
        "\n",
        "plt.gca().invert_yaxis()        # health decreases over time\n",
        "plt.title(f\"Health Index for Engine Unit {unit_id}\")\n",
        "plt.xlabel(\"Cycle\")\n",
        "plt.ylabel(\"Health Indicator\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yPcnZeNVf8qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def fft_energy(x):\n",
        "    \"\"\"\n",
        "    Sum of squared magnitudes of the FFT (a simple measure of signal power).\n",
        "    x is a 1D numpy array (rolling window of one sensor).\n",
        "    \"\"\"\n",
        "    # real FFT (only non-negative frequencies)\n",
        "    f = np.fft.rfft(x)\n",
        "    mag = np.abs(f)\n",
        "    return float(np.sum(mag ** 2))\n",
        "\n",
        "def fft_peak(x):\n",
        "    \"\"\"\n",
        "    Maximum magnitude in the FFT spectrum (dominant amplitude).\n",
        "    \"\"\"\n",
        "    f = np.fft.rfft(x)\n",
        "    mag = np.abs(f)\n",
        "    return float(np.max(mag))"
      ],
      "metadata": {
        "id": "mPPagqVqf9iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Important sensors (from your feature importance results)\n",
        "fft_sensors = [\"sensor_9\", \"sensor_18\", \"sensor_8\", \"sensor_12\", \"sensor_13\", \"sensor_7\", \"sensor_3\", \"sensor_4\", \"sensor_14\", \"sensor_2\"]\n",
        "\n",
        "# Rolling window (in cycles) for FFT computation\n",
        "FFT_WINDOW = 50  # you can try 30, 50, 100 etc."
      ],
      "metadata": {
        "id": "KOLz51aNf_4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll compute FFT features per engine (unit) over a rolling window\n",
        "for col in fft_sensors:\n",
        "    energy_col = f\"{col}_fft_energy\"\n",
        "    peak_col   = f\"{col}_fft_peak\"\n",
        "\n",
        "    # Rolling by engine unit\n",
        "    df[energy_col] = (\n",
        "        df.groupby(\"unit\")[col]\n",
        "          .rolling(window=FFT_WINDOW, min_periods=FFT_WINDOW)\n",
        "          .apply(fft_energy, raw=True)\n",
        "          .reset_index(level=0, drop=True)\n",
        "    )\n",
        "\n",
        "    df[peak_col] = (\n",
        "        df.groupby(\"unit\")[col]\n",
        "          .rolling(window=FFT_WINDOW, min_periods=FFT_WINDOW)\n",
        "          .apply(fft_peak, raw=True)\n",
        "          .reset_index(level=0, drop=True)\n",
        "    )\n",
        "\n",
        "# Optional: check new columns\n",
        "#df[[ \"unit\", \"cycle\" ] + [s + \"_fft_energy\" for s in fft_sensors]].head(60)"
      ],
      "metadata": {
        "id": "afxwZi3MgCFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unit_id = 4  # choose any engine ID that exists in your data\n",
        "plot_cols = [\"sensor_9\", \"sensor_9_fft_energy\", \"sensor_9_fft_peak\"]\n",
        "\n",
        "tmp = df[df[\"unit\"] == unit_id].reset_index(drop=True)\n",
        "\n",
        "ax = tmp.plot(x=\"cycle\", y=\"sensor_9\", label=\"sensor_9 (time)\", figsize=(10,6))\n",
        "ax2 = ax.twinx()\n",
        "tmp.plot(x=\"cycle\", y=\"sensor_9_fft_energy\", label=\"sensor_9_fft_energy\", ax=ax2, style='r--')\n",
        "tmp.plot(x=\"cycle\", y=\"sensor_9_fft_peak\", label=\"sensor_9_fft_peak\", ax=ax2, style='g:')\n",
        "\n",
        "ax.set_ylabel(\"Raw sensor_9\")\n",
        "ax2.set_ylabel(\"FFT features\")\n",
        "ax.set_title(f\"Engine {unit_id}: sensor_9 & FFT features\")\n",
        "ax.legend(loc=\"upper left\")\n",
        "ax2.legend(loc=\"upper right\")"
      ],
      "metadata": {
        "id": "MEXPckAjgEDk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}